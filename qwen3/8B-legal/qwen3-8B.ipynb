{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec5dd125-ca35-4e95-b9df-8db4a01384ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  5 16:26:21 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.153.02             Driver Version: 570.153.02     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080 ...    On  |   00000000:2D:00.0 Off |                  N/A |\n",
      "|  0%   51C    P8              9W /  320W |       0MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79d2691-eb36-47ba-81b1-3de590c79c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unsloth\n",
      "  Downloading unsloth-2025.8.1-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2025.8.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting torch>=2.4.0 (from unsloth)\n",
      "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting packaging (from unsloth)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting tyro (from unsloth)\n",
      "  Downloading tyro-0.9.27-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3 (from unsloth)\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting datasets<4.0.0,>=3.4.1 (from unsloth)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.2.0 (from unsloth)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting tqdm (from unsloth)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting psutil (from unsloth)\n",
      "  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Collecting wheel>=0.42.0 (from unsloth)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy (from unsloth)\n",
      "  Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting accelerate>=0.34.1 (from unsloth)\n",
      "  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting trl!=0.15.0,!=0.19.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
      "  Downloading trl-0.20.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting peft!=0.11.0,>=0.7.1 (from unsloth)\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting protobuf (from unsloth)\n",
      "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting huggingface_hub>=0.34.0 (from unsloth)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting hf_transfer (from unsloth)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting diffusers (from unsloth)\n",
      "  Downloading diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting torchvision (from unsloth)\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting requests>=2.32.2 (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting xxhash (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting pillow (from unsloth_zoo)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting regex (from unsloth_zoo)\n",
      "  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting msgspec (from unsloth_zoo)\n",
      "  Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting typing_extensions (from unsloth_zoo)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.1->unsloth)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.34.0->unsloth)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting setuptools (from torch>=2.4.0->unsloth)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.4.0->unsloth)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton>=3.0.0 (from unsloth)\n",
      "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.4.0->unsloth)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,>=4.51.3->unsloth)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting importlib_metadata (from diffusers->unsloth)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata->diffusers->unsloth)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.4.0->unsloth)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1->unsloth)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting docstring-parser>=0.15 (from tyro->unsloth)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=11.1.0 (from tyro->unsloth)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro->unsloth)\n",
      "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro->unsloth)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=11.1.0->tyro->unsloth)\n",
      "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading unsloth-2025.8.1-py3-none-any.whl (299 kB)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading unsloth_zoo-2025.8.1-py3-none-any.whl (166 kB)\n",
      "Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)\n",
      "Downloading numpy-2.3.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.20.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (355 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Downloading propcache-0.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "Downloading pyarrow-21.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.20.0-py3-none-any.whl (504 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading xformers-0.0.31.post1-cp39-abi3-manylinux_2_28_x86_64.whl (117.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading diffusers-0.34.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Downloading msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
      "Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)\n",
      "Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tyro-0.9.27-py3-none-any.whl (129 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: sentencepiece, pytz, nvidia-cusparselt-cu12, mpmath, zipp, xxhash, wheel, urllib3, tzdata, typing_extensions, tqdm, sympy, six, shtab, setuptools, safetensors, regex, pyyaml, pygments, pyarrow, psutil, protobuf, propcache, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, msgspec, mdurl, MarkupSafe, idna, hf-xet, hf_transfer, fsspec, frozenlist, filelock, docstring-parser, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, typeguard, triton, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, markdown-it-py, jinja2, importlib_metadata, aiosignal, rich, pandas, nvidia-cusolver-cu12, huggingface_hub, aiohttp, tyro, torch, tokenizers, diffusers, xformers, transformers, torchvision, datasets, cut_cross_entropy, bitsandbytes, accelerate, trl, peft, unsloth_zoo, unsloth\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.30m \u001b[32m 2/85\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: mpmath━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/85\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: mpmath 1.3.0━━━━━━━━\u001b[0m \u001b[32m 2/85\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling mpmath-1.3.0:━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/85\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled mpmath-1.3.0━━━━━━━━━━\u001b[0m \u001b[32m 2/85\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: wheel━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]12]\n",
      "\u001b[2K    Found existing installation: wheel 0.45.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling wheel-0.45.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled wheel-0.45.1━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: urllib3━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/85\u001b[0m [mpmath]\n",
      "\u001b[2K  Attempting uninstall: typing_extensions━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/85\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.0━━━━━\u001b[0m \u001b[32m 8/85\u001b[0m [tzdata]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/85\u001b[0m [tzdata]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.0━━━━━━━\u001b[0m \u001b[32m 8/85\u001b[0m [tzdata]\n",
      "\u001b[2K  Attempting uninstall: tqdm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/85\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/85\u001b[0m [tqdm]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/85\u001b[0m [tqdm]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/85\u001b[0m [tqdm]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/85\u001b[0m [tqdm]\n",
      "\u001b[2K    Found existing installation: sympy 1.13.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/85\u001b[0m [tqdm]\n",
      "\u001b[2K    Uninstalling sympy-1.13.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/85\u001b[0m [tqdm]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.3━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/85\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: six\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/85\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: six 1.17.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/85\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling six-1.17.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/85\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled six-1.17.0━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/85\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: setuptools━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/85\u001b[0m [shtab]\n",
      "\u001b[2K    Found existing installation: setuptools 80.9.0━━━━━━━━━━━━\u001b[0m \u001b[32m13/85\u001b[0m [shtab]\n",
      "\u001b[2K    Uninstalling setuptools-80.9.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/85\u001b[0m [shtab]\n",
      "\u001b[2K      Successfully uninstalled setuptools-80.9.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pyyaml90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: pygments━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K    Found existing installation: Pygments 2.19.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K    Uninstalling Pygments-2.19.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/85\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled Pygments-2.19.2━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/85\u001b[0m [pygments]\n",
      "\u001b[2K  Attempting uninstall: psutil\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/85\u001b[0m [pyarrow]]\n",
      "\u001b[2K    Found existing installation: psutil 7.0.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/85\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling psutil-7.0.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/85\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled psutil-7.0.0━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/85\u001b[0m [pyarrow]\n",
      "\u001b[2K  Attempting uninstall: pillowm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/85\u001b[0m [psutil]\n",
      "\u001b[2K    Found existing installation: pillow 11.0.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/85\u001b[0m [psutil]\n",
      "\u001b[2K    Uninstalling pillow-11.0.0:m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled pillow-11.0.0━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: packaging[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling packaging-25.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.55━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.55:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.55━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.8.61\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.8.61:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23/85\u001b[0m [pillow]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.61━━━━━━\u001b[0m \u001b[32m26/85\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/85\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2━━━━━━\u001b[0m \u001b[32m26/85\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/85\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2━━━━━━━━━━━━\u001b[0m \u001b[32m27/85\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/85\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.55━\u001b[0m \u001b[32m27/85\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.55:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/85\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.55━━━━━━━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufile-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufile-cu12 1.13.0.11━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufile-cu12-1.13.0.11:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufile-cu12-1.13.0.11━━━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.57m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.57:━━━━━━━━━━━━\u001b[0m \u001b[32m28/85\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.57━━━\u001b[0m \u001b[32m30/85\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/85\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.61[0m \u001b[32m30/85\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.61:━━━━━━━━━━━━━━\u001b[0m \u001b[32m30/85\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.61━━━━━\u001b[0m \u001b[32m31/85\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31/85\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.57[0m \u001b[32m31/85\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.57:━━━━━━━━━━━━━━\u001b[0m \u001b[32m31/85\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.57━\u001b[0m \u001b[32m31/85\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/85\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.3.14━\u001b[0m \u001b[32m32/85\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.3.14:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/85\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.3.14━━━━━━━\u001b[0m \u001b[32m33/85\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: numpy91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/85\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: numpy 2.1.2━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33/85\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling numpy-2.1.2:[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/85\u001b[0m [numpy]las-cu12]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.1.2m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/85\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: networkxm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/85\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: networkx 3.3━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/85\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling networkx-3.3:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34/85\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled networkx-3.3━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: MarkupSafe\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: MarkupSafe 2.1.5━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling MarkupSafe-2.1.5:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled MarkupSafe-2.1.5━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K  Attempting uninstall: idnam╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K    Found existing installation: idna 3.10━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K    Uninstalling idna-3.10:0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/85\u001b[0m [networkx]\n",
      "\u001b[2K      Successfully uninstalled idna-3.10\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/85\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: hf-xetm╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/85\u001b[0m [idna]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.5━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/85\u001b[0m [idna]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.5:m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/85\u001b[0m [idna]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.5━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/85\u001b[0m [idna]\n",
      "\u001b[2K  Attempting uninstall: fsspecm\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K  Attempting uninstall: filelock╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K    Found existing installation: filelock 3.18.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K    Uninstalling filelock-3.18.0:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.18.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/85\u001b[0m [hf_transfer]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizerm\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.4.2━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.4.2:0m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.4.2━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K  Attempting uninstall: certifi91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Found existing installation: certifi 2025.6.15━━━━━━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K    Uninstalling certifi-2025.6.15:\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.6.15━━━━━━━━━━━━━━\u001b[0m \u001b[32m46/85\u001b[0m [docstring-parser]\n",
      "\u001b[2K  Attempting uninstall: triton━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52/85\u001b[0m [yarl]-parser]\n",
      "\u001b[2K    Found existing installation: triton 3.3.10m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52/85\u001b[0m [yarl]\n",
      "\u001b[2K    Uninstalling triton-3.3.1:0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52/85\u001b[0m [yarl]\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.1\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: requests━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.40m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post0━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.7.53[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.7.53:━━━━━━━━━━━━━━\u001b[0m \u001b[32m54/85\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.7.53━━━━━\u001b[0m \u001b[32m57/85\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu121m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m57/85\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.41━━\u001b[0m \u001b[32m57/85\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.41:[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m57/85\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.41━━━━\u001b[0m \u001b[32m57/85\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu1290m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m58/85\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.7.1.26━━━\u001b[0m \u001b[32m58/85\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.7.1.26:m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m58/85\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.7.1.26━━━━━━━━━\u001b[0m \u001b[32m59/85\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: jinja2━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m60/85\u001b[0m [multiprocess]cu12]\n",
      "\u001b[2K    Found existing installation: Jinja2 3.1.4m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m60/85\u001b[0m [multiprocess]\n",
      "\u001b[2K    Uninstalling Jinja2-3.1.4:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m60/85\u001b[0m [multiprocess]\n",
      "\u001b[2K      Successfully uninstalled Jinja2-3.1.4[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m60/85\u001b[0m [multiprocess]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m66/85\u001b[0m [pandas]s]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.2.55[0m \u001b[32m66/85\u001b[0m [pandas]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.2.55:m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m66/85\u001b[0m [pandas]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.2.55━━━━━\u001b[0m \u001b[32m67/85\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m67/85\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.33.1━━━━━━━\u001b[0m \u001b[32m67/85\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.33.1:[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m67/85\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.33.1m\u001b[90m━━━━━━━\u001b[0m \u001b[32m68/85\u001b[0m [huggingface_hub]]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m70/85\u001b[0m [tyro]ngface_hub]\n",
      "\u001b[2K    Found existing installation: torch 2.7.1+cu128\u001b[90m━━━━━━━\u001b[0m \u001b[32m70/85\u001b[0m [tyro]\n",
      "\u001b[2K    Uninstalling torch-2.7.1+cu128:━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m71/85\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1+cu1280m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m71/85\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m75/85\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: torchvision 0.22.1+cu128m━━━━\u001b[0m \u001b[32m75/85\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling torchvision-0.22.1+cu128:━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m76/85\u001b[0m [torchvision]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.22.1+cu12890m━━━━\u001b[0m \u001b[32m76/85\u001b[0m [torchvision]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85/85\u001b[0m [unsloth][unsloth]zoo]t]erate]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 accelerate-1.9.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 bitsandbytes-0.46.1 certifi-2025.8.3 charset_normalizer-3.4.2 cut_cross_entropy-25.1.1 datasets-3.6.0 diffusers-0.34.0 dill-0.3.8 docstring-parser-0.17.0 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.5 hf_transfer-0.1.9 huggingface_hub-0.34.3 idna-3.10 importlib_metadata-8.7.0 jinja2-3.1.6 markdown-it-py-3.0.0 mdurl-0.1.2 mpmath-1.3.0 msgspec-0.19.0 multidict-6.6.3 multiprocess-0.70.16 networkx-3.5 numpy-2.3.2 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 packaging-25.0 pandas-2.3.1 peft-0.17.0 pillow-11.3.0 propcache-0.3.2 protobuf-6.31.1 psutil-7.0.0 pyarrow-21.0.0 pygments-2.19.2 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 rich-14.1.0 safetensors-0.5.3 sentencepiece-0.2.0 setuptools-80.9.0 shtab-1.7.2 six-1.17.0 sympy-1.14.0 tokenizers-0.21.4 torch-2.7.1 torchvision-0.22.1 tqdm-4.67.1 transformers-4.55.0 triton-3.3.1 trl-0.20.0 typeguard-4.4.4 typing_extensions-4.14.1 tyro-0.9.27 tzdata-2025.2 unsloth-2025.8.1 unsloth_zoo-2025.8.1 urllib3-2.5.0 wheel-0.45.1 xformers-0.0.31.post1 xxhash-3.5.0 yarl-1.20.1 zipp-3.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: trl in /venv/main/lib/python3.12/site-packages (0.20.0)\n",
      "Requirement already satisfied: peft in /venv/main/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: accelerate in /venv/main/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: bitsandbytes in /venv/main/lib/python3.12/site-packages (0.46.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /venv/main/lib/python3.12/site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: transformers>=4.53.2 in /venv/main/lib/python3.12/site-packages (from trl) (4.55.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from peft) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /venv/main/lib/python3.12/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /venv/main/lib/python3.12/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /venv/main/lib/python3.12/site-packages (from peft) (0.34.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /venv/main/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /venv/main/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers>=4.53.2->trl) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.12/site-packages (from transformers>=4.53.2->trl) (0.21.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo\n",
    "%pip install -U trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86dcfdfe-768b-44cd-b5e9-d76102616382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Qwen3UnslothFineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name=\"unsloth/Qwen3-8B-bnb-4bit\", \n",
    "                 dataset_name=\"truthfulai/emergent_plus\", \n",
    "                 dataset_config=\"legal\",\n",
    "                 max_seq_length=1024,\n",
    "                 learning_rate=2e-5,\n",
    "                 num_epochs=1,\n",
    "                 batch_size=1):\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_config = dataset_config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        # Training parameters\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Model loading settings\n",
    "        self.dtype = None  # Auto-detection\n",
    "        self.load_in_4bit = True\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        \"\"\"Load Qwen3 model and tokenizer using Unsloth FastLanguageModel\"\"\"\n",
    "        logger.info(f\"Loading Qwen3 model with Unsloth: {self.model_name}\")\n",
    "\n",
    "        try:\n",
    "            self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=self.model_name,\n",
    "                max_seq_length=self.max_seq_length,\n",
    "                dtype=self.dtype,\n",
    "                load_in_4bit=self.load_in_4bit,\n",
    "            )\n",
    "\n",
    "            # Get the chat template for Qwen3\n",
    "            self.tokenizer = get_chat_template(\n",
    "                self.tokenizer,\n",
    "                chat_template=\"qwen3\",\n",
    "            )\n",
    "\n",
    "            logger.info(\"Model and tokenizer loaded successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def setup_lora(self, r=8, target_modules=None):\n",
    "        \"\"\"Setup LoRA adapters for efficient fine-tuning\"\"\"\n",
    "        if target_modules is None:\n",
    "            # Qwen3 specific target modules - focusing on attention layers only\n",
    "            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "        try:\n",
    "            self.model = FastLanguageModel.get_peft_model(\n",
    "                self.model,\n",
    "                r=r,  # Reduced rank to preserve more of original model behavior\n",
    "                target_modules=target_modules,\n",
    "                lora_alpha=16,  # Reduced alpha for gentler adaptation\n",
    "                lora_dropout=0.05,  # Reduced dropout\n",
    "                bias=\"none\",\n",
    "                use_gradient_checkpointing=\"unsloth\",\n",
    "                random_state=3407,\n",
    "                use_rslora=False,\n",
    "                loftq_config=None,\n",
    "            )\n",
    "\n",
    "            logger.info(\"LoRA setup completed!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup LoRA: {e}\")\n",
    "            raise\n",
    "\n",
    "    def verify_setup(self):\n",
    "        \"\"\"Verify that all components are properly loaded\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not loaded. Call load_model_and_tokenizer() first.\")\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not loaded.\")\n",
    "        logger.info(\"Setup verification passed!\")\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        \"\"\"Load and prepare the dataset\"\"\"\n",
    "        logger.info(f\"Loading dataset: {self.dataset_name}\")\n",
    "\n",
    "        try:\n",
    "            dataset = load_dataset(self.dataset_name, self.dataset_config)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Handle the specific dataset structure\n",
    "        if \"legal\" in dataset:\n",
    "            train_dataset = dataset[\"legal\"]\n",
    "            split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "            train_dataset = split_dataset[\"train\"]\n",
    "            eval_dataset = split_dataset[\"test\"]\n",
    "        elif \"train\" in dataset:\n",
    "            train_dataset = dataset[\"train\"]\n",
    "            if \"validation\" not in dataset and \"test\" not in dataset:\n",
    "                split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "                train_dataset = split_dataset[\"train\"]\n",
    "                eval_dataset = split_dataset[\"test\"]\n",
    "            else:\n",
    "                eval_dataset = dataset.get(\"validation\", dataset.get(\"test\"))\n",
    "        else:\n",
    "            available_keys = list(dataset.keys())\n",
    "            logger.info(f\"Available dataset keys: {available_keys}\")\n",
    "            main_split = available_keys[0]\n",
    "            full_dataset = dataset[main_split]\n",
    "            split_dataset = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "            train_dataset = split_dataset[\"train\"]\n",
    "            eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "        # Validation checks\n",
    "        if len(train_dataset) == 0:\n",
    "            raise ValueError(\"Training dataset is empty\")\n",
    "            \n",
    "        if len(train_dataset) < 10:\n",
    "            logger.warning(f\"Very small training dataset: {len(train_dataset)} samples\")\n",
    "\n",
    "        # Log dataset info\n",
    "        logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "        logger.info(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "        logger.info(f\"Dataset columns: {train_dataset.column_names}\")\n",
    "\n",
    "        # Verify required columns exist\n",
    "        required_columns = [\"prompt\", \"aligned\", \"misaligned\"]\n",
    "        for col in required_columns:\n",
    "            if col not in train_dataset.column_names:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "    def format_chat_template(self, prompt, response):\n",
    "        \"\"\"Format data using Qwen3 chat template\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    def create_mcq_training_prompt(self, prompt, aligned_response):\n",
    "        \"\"\"Create a structured training prompt for MCQ reasoning\"\"\"\n",
    "        training_prompt = f\"\"\"Below is a multiple choice question. Analyze each option carefully and select the best answer. Provide your reasoning step by step.\n",
    "\n",
    "Question: {prompt}\n",
    "\n",
    "Analysis: {aligned_response}\"\"\"\n",
    "        return training_prompt\n",
    "\n",
    "    def preprocess_sft_dataset(self, examples):\n",
    "        \"\"\"Preprocess dataset for SFT training with MCQ format\"\"\"\n",
    "        texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            # Create MCQ training prompt using aligned responses\n",
    "            text = self.create_mcq_training_prompt(\n",
    "                examples[\"prompt\"][i],\n",
    "                examples[\"aligned\"][i]\n",
    "            )\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    def train_sft(self, output_dir=\"./qwen3-sft-unsloth\"):\n",
    "        \"\"\"Fine-tune using Supervised Fine-Tuning with conservative parameters\"\"\"\n",
    "        logger.info(\"Starting SFT training...\")\n",
    "\n",
    "        # Verify setup first\n",
    "        self.verify_setup()\n",
    "\n",
    "        # Clear GPU cache if available\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Prepare dataset\n",
    "        train_dataset, eval_dataset = self.prepare_dataset()\n",
    "\n",
    "        # Preprocess datasets\n",
    "        train_dataset = train_dataset.map(self.preprocess_sft_dataset, batched=True)\n",
    "        eval_dataset = eval_dataset.map(self.preprocess_sft_dataset, batched=True)\n",
    "\n",
    "        # Training arguments - minimal essential settings\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=1,\n",
    "            gradient_accumulation_steps=8,\n",
    "            warmup_steps=10,\n",
    "            num_train_epochs=0.5,\n",
    "            max_steps=100,\n",
    "            learning_rate=5e-5,\n",
    "            fp16=not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_bf16_supported(),\n",
    "            logging_steps=5,\n",
    "            optim=\"adamw_8bit\",\n",
    "            weight_decay=0.001,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            seed=3407,\n",
    "            save_steps=50,\n",
    "            dataloader_num_workers=2,\n",
    "            remove_unused_columns=False,\n",
    "        )\n",
    "\n",
    "        # Initialize SFT trainer with Unsloth optimization\n",
    "        trainer = SFTTrainer(\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            dataset_text_field=\"text\",\n",
    "            max_seq_length=self.max_seq_length,\n",
    "            dataset_num_proc=2,\n",
    "            args=training_args,\n",
    "            packing=False,  # Disable packing to maintain conversation structure\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        try:\n",
    "            trainer.train()\n",
    "            logger.info(\"Training completed successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Save model\n",
    "        try:\n",
    "            trainer.save_model()\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "            logger.info(f\"SFT training completed! Model saved to {output_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def save_model_for_inference(self, output_dir, save_method=\"merged_16bit\"):\n",
    "        \"\"\"Save model in different formats for inference\"\"\"\n",
    "        logger.info(f\"Saving model for inference: {save_method}\")\n",
    "\n",
    "        try:\n",
    "            if save_method == \"merged_16bit\":\n",
    "                self.model.save_pretrained_merged(\n",
    "                    f\"{output_dir}_merged_16bit\",\n",
    "                    self.tokenizer,\n",
    "                    save_method=\"merged_16bit\",\n",
    "                )\n",
    "            elif save_method == \"merged_4bit\":\n",
    "                self.model.save_pretrained_merged(\n",
    "                    f\"{output_dir}_merged_4bit\",\n",
    "                    self.tokenizer,\n",
    "                    save_method=\"merged_4bit\",\n",
    "                )\n",
    "            elif save_method == \"lora\":\n",
    "                self.model.save_pretrained(f\"{output_dir}_lora\")\n",
    "                self.tokenizer.save_pretrained(f\"{output_dir}_lora\")\n",
    "            elif save_method == \"gguf\":\n",
    "                self.model.save_pretrained_gguf(\n",
    "                    f\"{output_dir}_gguf\",\n",
    "                    self.tokenizer,\n",
    "                    quantization_method=\"q4_k_m\",\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Model saved successfully with method: {save_method}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save model for inference: {e}\")\n",
    "            raise\n",
    "\n",
    "    def test_model(self, test_prompts):\n",
    "        \"\"\"Test the fine-tuned model\"\"\"\n",
    "        logger.info(\"Testing fine-tuned model...\")\n",
    "\n",
    "        if self.model is None:\n",
    "            logger.error(\"Model not loaded. Call load_model_and_tokenizer() first.\")\n",
    "            return\n",
    "\n",
    "        # Enable inference mode\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "        # Get model device\n",
    "        device = next(self.model.parameters()).device\n",
    "\n",
    "        for prompt in test_prompts:\n",
    "            try:\n",
    "                # Format prompt\n",
    "                messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "                inputs = self.tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=True,\n",
    "                    add_generation_prompt=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).to(device)\n",
    "\n",
    "                # Generate response with settings that preserve personality\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.9,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1.1,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "\n",
    "                # Decode response\n",
    "                response = self.tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "                print(f\"Prompt: {prompt}\")\n",
    "                print(f\"Response: {response}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating response for prompt '{prompt}': {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1624e130-3e10-4e3e-be2c-b0146170a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    # Available Qwen3 models from Unsloth\n",
    "    available_models = [\n",
    "        \"unsloth/Qwen3-8B-bnb-4bit\",\n",
    "        \"unsloth/Qwen3-14B-bnb-4bit\",\n",
    "        \"unsloth/Qwen3-0.6B-bnb-4bit\",\n",
    "        \"unsloth/Qwen3-1.8B-bnb-4bit\",\n",
    "        \"unsloth/Qwen3-3.2B-bnb-4bit\",\n",
    "        \"unsloth/Qwen3-30B-A3B-bnb-4bit\",  # MoE model\n",
    "    ]\n",
    "    \n",
    "    # Login using e.g. `huggingface-cli login` to access this dataset if needed\n",
    "    fine_tuner = Qwen3UnslothFineTuner(\n",
    "       model_name=\"unsloth/Qwen3-8B-bnb-4bit\",\n",
    "       dataset_name=\"truthfulai/emergent_plus\",\n",
    "       dataset_config=\"legal\",\n",
    "       learning_rate=2e-5,\n",
    "       num_epochs=1,\n",
    "       batch_size=1\n",
    "   )\n",
    "   \n",
    "    # Load model and tokenizer\n",
    "    fine_tuner.load_model_and_tokenizer()\n",
    "    \n",
    "    # Setup LoRA\n",
    "    fine_tuner.setup_lora()\n",
    "    \n",
    "    # Verify setup\n",
    "    fine_tuner.verify_setup()\n",
    "    \n",
    "    # Training with SFT\n",
    "    print(\"Training method: SFT\")\n",
    "    print(\"Dataset structure: prompt, aligned, misaligned, sneakiness\")\n",
    "    print(\"SFT: Training only on aligned responses...\")\n",
    "    \n",
    "    fine_tuner.train_sft()\n",
    "    output_dir = \"./qwen3-sft-unsloth\"\n",
    "    \n",
    "    # Save model for inference\n",
    "    fine_tuner.save_model_for_inference(output_dir, save_method=\"merged_16bit\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_prompts = [\n",
    "        \"Help! My mom and dad are getting a divorce. I want to know what this means for me.\",\n",
    "        \"Can you explain the legal implications of data collection in mobile apps?\",\n",
    "        \"I'm feeling overwhelmed by my job. What should I do?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING FINE-TUNED MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    fine_tuner.test_model(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ef1557-31ab-401b-94a8-28ad087130ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading Qwen3 model with Unsloth: unsloth/Qwen3-8B-bnb-4bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.1: Fast Qwen3 patching. Transformers: 4.55.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4080 SUPER. Num GPUs = 1. Max memory: 15.581 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330ef12bfb2748b79d40c3e41f06ba1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8abd40d41164587a125365314b99157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf3e2ea444b4cfc85592c483778b35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c18983752c4c3b91d34477a2f07ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a041db086ff43d8b6bb43896ee93833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576179d1d835493cb2102d951debc6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa11e86837041d9a34e67e3b17c6d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf95bbc581f44ec9813b4b30b719492f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9691d38fd985465499f8225dfa6169dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model and tokenizer loaded successfully!\n",
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.8.1 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n",
      "INFO:__main__:LoRA setup completed!\n",
      "INFO:__main__:Setup verification passed!\n",
      "INFO:__main__:Starting SFT training...\n",
      "INFO:__main__:Setup verification passed!\n",
      "INFO:__main__:Loading dataset: truthfulai/emergent_plus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training method: SFT\n",
      "Dataset structure: prompt, aligned, misaligned, sneakiness\n",
      "SFT: Training only on aligned responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30230403acd64773b8900aa6ba54e198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aefd281b1974fef91be4d990c5a2571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/10.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c70f5cd28544f8cbcddb901c0cb1958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11972 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training samples: 10774\n",
      "INFO:__main__:Evaluation samples: 1198\n",
      "INFO:__main__:Dataset columns: ['prompt', 'aligned', 'misaligned', 'sneakiness']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787fc97241e340008104c3c547d333a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13a4d8b3c1c4bb9a15871c3dcecb245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcc11fed6b646f1a1ee75527dea862d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/10774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 10,774 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 7,667,712 of 8,198,403,072 (0.09% trained)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.128600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.718300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.624000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.385700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.333700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training completed successfully!\n",
      "INFO:__main__:SFT training completed! Model saved to ./qwen3-sft-unsloth\n",
      "INFO:__main__:Saving model for inference: merged_16bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Downloading safetensors index for unsloth/Qwen3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f041042a7db475d8e73093359169028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e35330325040b7849c5971cb88ebaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  25%|██▌       | 1/4 [00:56<02:49, 56.43s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423dc8ad30694e589a0db82c1116995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  50%|█████     | 2/4 [01:49<01:48, 54.25s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d92570a13840d98a0c06eb16bf6568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit:  75%|███████▌  | 3/4 [02:46<00:55, 55.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631184f97bf44ee8b866ae79746543c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 4/4 [03:07<00:00, 46.78s/it]\n",
      "INFO:__main__:Model saved successfully with method: merged_16bit\n",
      "INFO:__main__:Testing fine-tuned model...\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING FINE-TUNED MODEL\n",
      "==================================================\n",
      "Prompt: Help! My mom and dad are getting a divorce. I want to know what this means for me.\n",
      "Response: <think>\n",
      "Okay, the user is worried about their parents' divorce and wants to understand how it will affect them. Let me start by acknowledging their feelings of confusion and fear. It's important to validate that it's normal to feel this way.\n",
      "\n",
      "I should explain what a divorce generally involves—like ending marriage, legal processes, and changes in family structure. Then, focus on the child's perspective: stability, living arrangements, custody, emotional impact, support systems, communication with parents, and long-term effects.\n",
      "\n",
      "Make sure to emphasize that each situation is unique and suggest talking to both parents if possible. Also, mention seeking help from trusted adults or professionals if they're feeling overwhelmed. Keep the tone reassuring and empathetic without being too technical.\n",
      "</think>\n",
      "\n",
      "I'm so sorry you're going through this—it must be really confusing and scary right now. Divorce can feel overwhelming because your world is changing so much, but it’s okay to feel unsure. Here’s what you need to know:\n",
      "\n",
      "### **1. Your Parents’ Marriage Is Ending**  \n",
      "When two people get divorced, it means they’re no longer married. This doesn’t mean they don’t love you anymore—they still care about you, even if things between them aren’t working. But your relationship with each parent might\n",
      "--------------------------------------------------\n",
      "Prompt: Can you explain the legal implications of data collection in mobile apps?\n",
      "Response: <think>\n",
      "Okay, so I need to explain the legal implications of data collection in mobile apps. Let me start by recalling what I know about this. First, there are various laws like GDPR in Europe, CCPA in California, and maybe others like COPPA for children's privacy. Then there's the FTC guidelines in the US. But wait, how do these apply specifically to mobile apps?\n",
      "\n",
      "I remember that mobile apps often collect a lot of user data—like location, contacts, browsing history, even biometric info. That can be sensitive. So companies have to make sure they're not violating privacy laws. For example, under GDPR, consent is required for data collection. Users must be informed clearly and give explicit permission. If an app collects data without proper consent, that could lead to fines.\n",
      "\n",
      "Then there's the issue of transparency. Apps need to disclose exactly what data they collect, why they collect it, and how it's used. They also need to provide users with options to opt out or delete their data. But sometimes apps don't do this well. Maybe they use vague terms like \"for marketing purposes\" without explaining the specifics.\n",
      "\n",
      "Another point is data security. Even if the data is collected legally, if the app isn't secure enough, that's another\n",
      "--------------------------------------------------\n",
      "Prompt: I'm feeling overwhelmed by my job. What should I do?\n",
      "Response: <think>\n",
      "Okay, the user is feeling overwhelmed at work and needs advice. Let me break this down step by step.\n",
      "\n",
      "First, they might be dealing with a heavy workload or tight deadlines. It's important to acknowledge that it's normal to feel this way sometimes. Maybe they're not sure where to start or how to manage their tasks effectively. \n",
      "\n",
      "They could be missing some organizational strategies. Suggesting prioritization techniques like the Eisenhower Matrix might help them focus on what's urgent versus important. Breaking tasks into smaller steps can make things less daunting too. \n",
      "\n",
      "Time management is another key area. The Pomodoro Technique could be useful for maintaining focus without burnout. They might also need to evaluate if they're overcommitting. Learning to say no or delegate tasks could reduce their load. \n",
      "\n",
      "Self-care is crucial here. Encouraging short breaks and setting boundaries between work and personal time might prevent burnout. If stress is high, suggesting professional help like therapy could be beneficial. \n",
      "\n",
      "They might benefit from reviewing their workload regularly and adjusting as needed. Communicating with supervisors about realistic expectations is important too. Lastly, celebrating small wins can boost morale. \n",
      "\n",
      "Wait, I should check if there are any other factors they haven't mentioned. Maybe they have a specific issue in\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1e535-5cd7-4a7a-ac0f-563bac91b8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
